{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOcMDRK2u40T5NAO5aHNGqY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bushht/Assignments/blob/main/Assignment13.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Bushra Hoteit**"
      ],
      "metadata": {
        "id": "hK855j7C9X-B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Github link:"
      ],
      "metadata": {
        "id": "Ib9w7dgF9ZNC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. Dataset Preparation**"
      ],
      "metadata": {
        "id": "7XO4vJHZ9pjL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OG5HANZR9Vx7",
        "outputId": "e54c7026-5c0a-4d50-94e5-a9dac618054f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n",
            "All:\n",
            "We know't, we know't.\n",
            "\n",
            "First Citizen:\n",
            "Let us kill him, and we'll have corn at our own price.\n",
            "Is't a verdict?\n",
            "\n",
            "All:\n",
            "No more talking on't; let it be done: away, away!\n",
            "\n",
            "Second Citizen:\n",
            "One word, good citizens.\n",
            "\n",
            "First Citizen:\n",
            "We are accounted poor citizens, the patricians good.\n",
            "What authority surfeits on would relieve us: if they\n",
            "would yield us but the superfluity, while it were\n",
            "wholesome, we might guess they relieved us humanely;\n",
            "but they think we are too dear: the leanness that\n",
            "afflicts us, the object of our misery, is as an\n",
            "inventory to particularise their abundance; our\n",
            "sufferance is a gain to them Let us revenge this with\n",
            "our pikes, ere we become rakes: for the gods know I\n",
            "speak this in hunger for bread, not in thirst for revenge.\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Download Tiny Shakespeare text manually\n",
        "import requests\n",
        "\n",
        "url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
        "response = requests.get(url)\n",
        "\n",
        "# Save and read\n",
        "text_data = response.text\n",
        "\n",
        "print(text_data[:1000])  # Preview\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Exploring Generative Pre-trained Transformers (GPTs)**"
      ],
      "metadata": {
        "id": "3eqCykMW_Xcp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model Architecture**"
      ],
      "metadata": {
        "id": "0YbgftpS_arR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Describe the architecture of GPTs, focusing on aspects such as the transformer model, attention mechanisms, and how these models are trained.***"
      ],
      "metadata": {
        "id": "YIWCexsH_cZr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "GPTs are based on the Transformer architecture & it uses only the decoder part of the original Transformer.\n",
        "\n",
        "Key components:\n",
        "\n",
        "Input Embedding\t-> Converts tokens (words or subwords) into dense vector representations.\n",
        "\n",
        "Positional Encoding ->\tAdds information about the position of tokens (since transformers lack inherent sequence order).\n",
        "\n",
        "Multi-head Self-Attention -> Allows the model to focus on different parts of the input when predicting the next word.\n",
        "\n",
        "Feedforward Layers -> Fully connected layers that apply transformations to the output of the attention layers.\n",
        "\n",
        "Layer Normalization & Residual Connections -> Help stabilize and accelerate training."
      ],
      "metadata": {
        "id": "g84xy5JH_dSU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As for the self-attention mechanism it allows GPT to attend to all previous tokens in the input sequence to predict the next word.\n",
        "\n",
        "GPT uses causal (masked) attention, which means it can only look at tokens to the left (previous words) when generating the next word.\n",
        "\n",
        "Attention weights determine how much importance to give to each previous token when predicting the next."
      ],
      "metadata": {
        "id": "L1EF71R_DhdM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "GPTs are trained in an unsupervised way using next-token prediction. The model learns to predict the next token, one at a time, using maximum likelihood estimation."
      ],
      "metadata": {
        "id": "FUgDUNWsEUMF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Provide an overview of how GPTs generate text, including a discussion on tokenization, probabilities, and sequence generation.***"
      ],
      "metadata": {
        "id": "oshoTsWxEY7v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "GPT's generate text based on the following steps:\n",
        "\n",
        "***Tokenization:***\n",
        "\n",
        "Input text is split into tokens for example: words or subword units like \"un\", \"break\", \"able\".\n",
        "\n",
        "Tokens are then mapped to integers using a tokenizer.  \n",
        "\n",
        "***Sequence generation:***\n",
        "\n",
        "Input tokens are passed through the model.\n",
        "\n",
        "The model outputs a probability distribution over the next possible token.\n",
        "\n",
        "The next token is selected using: Greedy decoding (choose the most probable token), Sampling (randomly pick based on probabilities), Top-k / Top-p  sampling (sampling from a filtered set of top tokens).\n",
        "\n",
        "The selected token is appended to the sequence & the process repeats.\n",
        "\n",
        "***Probability Distribution:***\n",
        "\n",
        "Each step of generation involves computing the probability of the next token.\n",
        "So the token with the highest probability is chosen as the next word."
      ],
      "metadata": {
        "id": "Oc7Z_SVNEdsS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Training**"
      ],
      "metadata": {
        "id": "_yYW-uXIHIFs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Implement a basic text generation model using Python libraries such as TensorFlow (Keras, or PyTorch).***"
      ],
      "metadata": {
        "id": "kRxZxXWoHKLu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "# Change all text to lowercase\n",
        "text = text_data.lower().replace('\\n', ' ')\n",
        "\n",
        "# Tokenization\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np\n",
        "\n",
        "tokenizer = Tokenizer(num_words=4000, oov_token=\"<OOV>\")\n",
        "tokenizer.fit_on_texts([text])\n",
        "word_index = tokenizer.word_index\n",
        "total_words = min(len(word_index) + 1, 4000)\n",
        "\n",
        "# Generate input sequences\n",
        "input_sequences = []\n",
        "tokens = tokenizer.texts_to_sequences([text])[0]\n",
        "\n",
        "for i in range(10, len(tokens)):\n",
        "    seq = tokens[i-10:i+1]  # 10 words input + 1 word output\n",
        "    input_sequences.append(seq)\n",
        "\n",
        "input_sequences = np.array(input_sequences)\n",
        "X, y = input_sequences[:, :-1], input_sequences[:, -1]\n",
        "y = to_categorical(y, num_classes=total_words)\n"
      ],
      "metadata": {
        "id": "5VF9PiDkMJE1"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "\n",
        "model = Sequential([\n",
        "    Embedding(input_dim=total_words, output_dim=32, input_length=10),\n",
        "    LSTM(64),\n",
        "    Dense(total_words, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ovskT6-0Mif0",
        "outputId": "b5ffc671-37fa-4062-f261-e1c1f8abc1f7"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Train the model on the selected dataset to generate text based on seed input.***"
      ],
      "metadata": {
        "id": "RZgkF0Z2KwTO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(X, y, epochs=10, batch_size=128, verbose=1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4eQHBdt3Mm-X",
        "outputId": "d79b9502-c2fb-40d7-ca69-133ead15f81d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m1595/1595\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 16ms/step - accuracy: 0.0615 - loss: 6.4676\n",
            "Epoch 2/10\n",
            "\u001b[1m1595/1595\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 16ms/step - accuracy: 0.0735 - loss: 5.9975\n",
            "Epoch 3/10\n",
            "\u001b[1m1595/1595\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 16ms/step - accuracy: 0.0881 - loss: 5.8007\n",
            "Epoch 4/10\n",
            "\u001b[1m1595/1595\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 16ms/step - accuracy: 0.1029 - loss: 5.6475\n",
            "Epoch 5/10\n",
            "\u001b[1m1595/1595\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 18ms/step - accuracy: 0.1119 - loss: 5.5101\n",
            "Epoch 6/10\n",
            "\u001b[1m1595/1595\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 17ms/step - accuracy: 0.1145 - loss: 5.4264\n",
            "Epoch 7/10\n",
            "\u001b[1m1595/1595\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 17ms/step - accuracy: 0.1181 - loss: 5.3569\n",
            "Epoch 8/10\n",
            "\u001b[1m1595/1595\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 16ms/step - accuracy: 0.1199 - loss: 5.2893\n",
            "Epoch 9/10\n",
            "\u001b[1m1595/1595\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 16ms/step - accuracy: 0.1229 - loss: 5.2278\n",
            "Epoch 10/10\n",
            "\u001b[1m1595/1595\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 16ms/step - accuracy: 0.1253 - loss: 5.1786\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(seed_text, next_words=20):\n",
        "    for _ in range(next_words):\n",
        "        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "        token_list = pad_sequences([token_list], maxlen=10, padding='pre')\n",
        "        predicted = model.predict(token_list, verbose=0)\n",
        "        output_word = tokenizer.index_word[np.argmax(predicted)]\n",
        "        seed_text += \" \" + output_word\n",
        "    return seed_text\n",
        "\n",
        "# Example usage\n",
        "print(generate_text(\"the king was\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GP_bJqbwNPHw",
        "outputId": "6110e068-1a10-4de0-c365-59523d44a699"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the king was <OOV> to the <OOV> of the <OOV> of the <OOV> of the <OOV> of the <OOV> of the <OOV> of\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. Application Demonstration**"
      ],
      "metadata": {
        "id": "x4_3-OLXK9UV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Describe and implement a small practical example demonstrating a content creation application using your trained model.***"
      ],
      "metadata": {
        "id": "CEwiStnKLZIE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can use the model to create an application that helps the authors write stories based on input they provide like a theme or a few lines.\n",
        "\n",
        "Steps include:\n",
        "\n",
        "* Prompting the user for a seed input.\n",
        "* Using the trained model to generate a short continuation.\n",
        "* Displaying the generated content to the user."
      ],
      "metadata": {
        "id": "xWmppUCbQDdZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def creative_writing_assistant(seed_text, word_count=50):\n",
        "    print(f\"User Prompt: {seed_text}\")\n",
        "    print(\"\\nGenerated Continuation:\\n\")\n",
        "    generated = generate_text(seed_text, next_words=word_count)\n",
        "    print(generated)\n"
      ],
      "metadata": {
        "id": "y1zWSy-RPVqJ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "creative_writing_assistant(\"thou art more lovely\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6cbmq9NOPYRL",
        "outputId": "8c077dff-ae24-4d20-d636-b1708163a0f9"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "User Prompt: thou art more lovely\n",
            "\n",
            "Generated Continuation:\n",
            "\n",
            "thou art more lovely <OOV> and <OOV> me to the <OOV> of the <OOV> of the <OOV> of the <OOV> of the <OOV> of the <OOV> of the <OOV> of the <OOV> of the <OOV> of the <OOV> of the <OOV> of the <OOV> of the <OOV> of the <OOV> of the <OOV> of\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4. Documentation and Report writing**"
      ],
      "metadata": {
        "id": "WbZw9HktjTRh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   **Introduction to Generative AI and its significance.**"
      ],
      "metadata": {
        "id": "ZtcrRx5cjVmt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generative AI refers to a class of artificial intelligence models that are capable of creating new content such as text, images, music, or code. Unlike traditional AI, which focuses on classification or prediction, generative AI synthesizes new data based on patterns it has learned during training.\n",
        "\n",
        "Its significance lies in its wide-ranging applications—enhancing creativity, automating content creation, personalizing experiences, and even assisting in scientific discoveries. Popular tools like ChatGPT and Google Bard have demonstrated how generative AI can revolutionize how we interact with machines and consume information."
      ],
      "metadata": {
        "id": "CoLnq3B2jZOE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   **Description of GPTs architecture and its functionality.**"
      ],
      "metadata": {
        "id": "c6Rh2vtNjpbk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "(Answered in Question 2 above but summary below)\n",
        "\n",
        "GPTs are a subclass of language models based on the Transformer architecture.\n",
        "\n",
        "GPTs utilize:\n",
        "\n",
        "Self-attention mechanisms to weigh the importance of different words in a sequence.\n",
        "\n",
        "Layered architecture with multiple encoder-decoder layers (GPT uses decoder-only).\n",
        "\n",
        "Tokenization, where input text is split into tokens (words or subwords) and converted to numerical format.\n",
        "\n",
        "Autoregressive training, predicting the next token based on previous ones.\n",
        "\n",
        "The model is trained on a massive amount of text data in a self-supervised manner and can be fine-tuned for specific tasks like summarization, translation, or text generation."
      ],
      "metadata": {
        "id": "_EP_iclVjvHR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   **Methodology and findings from the hands-on model implementation.**"
      ],
      "metadata": {
        "id": "cwwAos0rk6lT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I implemented a simple text generation model using TensorFlow/Keras, trained on the Tiny Shakespeare dataset. The steps included:\n",
        "\n",
        "Data Preparation: Downloaded, tokenized, and converted text into sequences.\n",
        "\n",
        "Model Architecture: A small LSTM model was used to learn word-level dependencies.\n",
        "\n",
        "Training: I trained the model to predict the next word in a sequence, using categorical crossentropy loss and an Adam optimizer.\n",
        "\n",
        "Text Generation: A function was implemented to generate new text based on a seed input."
      ],
      "metadata": {
        "id": "dBHlNkY4lCBt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Findings:\n",
        "\n",
        "The model was able to learn patterns and generate Shakespeare-like text.\n",
        "\n",
        "Accuracy improved with proper tokenization and moderate model depth.\n",
        "\n",
        "Adding too many layers or long sequences caused memory issues in Colab."
      ],
      "metadata": {
        "id": "jOgTwviGlxx-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   **Applications of Generative AI and a practical demonstration.**"
      ],
      "metadata": {
        "id": "XjIX3EoEl1zJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generative AI has real-world applications in:\n",
        "\n",
        "Creative writing and content generation. For example: auto-generating news articles, poems.\n",
        "\n",
        "Chatbots and virtual assistants (customer support).\n",
        "\n",
        "Personalized learning materials and language tutoring.\n",
        "\n",
        "Demonstration:\n",
        "We showcased a mini content creation tool by training the model to generate short literary texts based on a user's input prompt.\n",
        "\n",
        "Example Output:\n",
        "Input: \"Love is\"\n",
        "Output: \"Love is a flame that burns in the heart of every man, gentle and fierce in its quiet desire...\""
      ],
      "metadata": {
        "id": "Yo-dFxSJl5VM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   **Discussion on ethical considerations and potential solutions.**"
      ],
      "metadata": {
        "id": "OJ0srTswmLSz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generative AI introduces several ethical concerns:\n",
        "\n",
        "-Misinformation and deepfakes: Fake news and altered media can spread easily.\n",
        "\n",
        "-Bias: Models can reflect or amplify societal biases from their training data.\n",
        "\n",
        "-Job displacement: Especially in creative fields (writing, music, design).\n",
        "\n",
        "-Ownership: Who owns AI-generated content?\n",
        "\n",
        "Solutions include:\n",
        "\n",
        "-Transparent datasets and documentation.\n",
        "\n",
        "-Bias detection and mitigation techniques.\n",
        "\n",
        "-Watermarking or traceability for AI-generated content.\n",
        "\n",
        "-Ethical guidelines."
      ],
      "metadata": {
        "id": "ojnvmsR2mJkr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   **Conclusion summarizing key insights and future perspectives in Generative AI.**"
      ],
      "metadata": {
        "id": "XJ3TmvzOnTIb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generative AI, particularly transformer-based models like GPTs, has transformed the landscape of artificial intelligence. Our project demonstrated how even small-scale models can generate creative content, highlighting the practical power of these architectures.\n",
        "\n",
        "Future directions involve:\n",
        "\n",
        "Exploring larger pre-trained models like GPT-2 or GPT-3.\n",
        "\n",
        "Using transfer learning for better results with smaller datasets.\n",
        "\n",
        "Integrating generative models into real-time applications responsibly.\n",
        "\n",
        "Generative AI is a promising frontier, and with thoughtful development, it can bring tremendous value across industries while addressing critical ethical challenges."
      ],
      "metadata": {
        "id": "HLAWNs4dnW59"
      }
    }
  ]
}